# üßæ Ficha T√©cnica - Juanito Software 2025

## üß† Descripci√≥n General

**Juanito Software 2025** es un sistema automatizado de generaci√≥n, validaci√≥n, ejecuci√≥n y documentaci√≥n de c√≥digo Python utilizando modelos de lenguaje basados en LLMs como **CodeLlama-GPTQ** y **CodeGemma**. Est√° dise√±ado para crear scripts funcionales, corregir errores autom√°ticamente, validar su ejecuci√≥n y documentar el c√≥digo generado.

## üë®‚Äçüíª Desarrollador

**GPTDevTeam**

---

## üß± Componentes Principales

- **CodeLlama-7B-GPTQ**: Modelo LLM cuantizado para generaci√≥n de c√≥digo.
- **CodeGemma (Ollama)**: LLM para documentaci√≥n autom√°tica v√≠a API local.
- **Transformers** (HuggingFace): Tokenizaci√≥n y manipulaci√≥n del modelo.
- **Triton / Torch / CUDA**: Infraestructura para ejecuci√≥n en GPU.
- **Subprocess / threading**: Control de ejecuci√≥n del c√≥digo generado.

---

## ‚öôÔ∏è Requisitos del Sistema

### Dependencias de Python:

```bash
transformers
torch
triton
requests
textwrap
huggingface_hub
```

### Requisitos del sistema:

- Python 3.8+
- GPU compatible con CUDA
- [Ollama](https://ollama.com/) corriendo localmente en `localhost:11434`
- Memoria recomendada: 16 GB RAM / 12 GB VRAM
- Acceso a internet (opcional, si se usan modelos locales ya descargados)

---

## üß© Estructura del Programa

- `main()`: Orquestador general del flujo de trabajo.
- `codellama_generate(prompt)`: Generaci√≥n de c√≥digo a partir de una instrucci√≥n.
- `build_codellama_prompt(code)`: Generaci√≥n de prompts de refactorizaci√≥n.
- `ejecutar_codigo_py(path)`: Ejecuta el c√≥digo generado y verifica su funcionamiento.
- `extraer_codigo_puro(texto)`: Limpia la salida de los LLMs para extraer solo el c√≥digo.
- `documentar_codigo(codigo)`: Usa CodeGemma para comentar el c√≥digo.
- `limpiar_docstring_inicial(codigo)`: Elimina docstrings incorrectos o mal formateados.

---

## üîÅ Flujo de Ejecuci√≥n

1. Se genera un script Python a partir de una descripci√≥n en lenguaje natural.
2. El c√≥digo se limpia y formatea.
3. Se corrige iterativamente con un m√°ximo de 4 intentos si contiene errores.
4. Se ejecuta en tiempo real y se monitorea por errores.
5. Si la ejecuci√≥n es exitosa, se documenta autom√°ticamente.
6. El c√≥digo final se guarda como `CodigoFinal.py`.

---

## üì¶ Salidas

- `codigo_actual.py`: C√≥digo intermedio a ejecutar/testear.
- `CodigoFinal.py`: C√≥digo funcional y documentado.
- Logs en consola: estado de cada intento de ejecuci√≥n y errores detectados.

---

## üõ°Ô∏è Validaci√≥n de Ejecuci√≥n

El c√≥digo generado se ejecuta autom√°ticamente. Si contiene errores, se depura con ayuda del propio modelo, reutilizando el mensaje de error como parte del nuevo prompt.

---

## üåê Conectividad

- Ollama debe estar corriendo localmente.
- El modelo "codegemma:7b-instruct" debe estar disponible para generar documentaci√≥n.

---

## üìö Licencias y Cr√©ditos

- Modelos LLM proporcionados por [TheBloke](https://huggingface.co/TheBloke) y [Google](https://github.com/google/codegemma).
- Uso de Transformers y HuggingFace Hub bajo licencia Apache 2.0.
- Este software es de uso experimental/educativo y no garantiza c√≥digo seguro en entornos de producci√≥n.

---

## üèÅ Ejecuci√≥n

```bash
python main.py
```

---

## üö® Notas de Seguridad

- **No ejecutar el c√≥digo generado sin revisi√≥n manual previa en entornos sensibles.**
- **El sistema puede generar c√≥digo no seguro si el prompt lo induce.**

---
